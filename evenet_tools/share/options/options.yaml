Training:
  # Total plan epochs for optimizer decay
  total_epochs: 100
  # Trainer will stop at this epoch number
  epochs: 100

  model_checkpoint_save_path: "."
  model_checkpoint_load_path: null

  pretrain_model_load_path: null

  learning_rate: &lr 0.00005
  learning_rate_factor: &lr_factor 1.0
  learning_rate_warm_up_factor: &lr_warm 1.0
  weight_decay: &wd 0.001
  decoupled_weight_decay: &decouple_wd true

  diffusion_every_n_epochs: 20 # how often to run diffusion
  diffusion_every_n_steps: 5 # how often to run diffusion inside the valid epoch
  eval_metrics_every_n_epochs: 1 # how often to run evaluation metrics

  apply_event_weight: false # whether to apply event weight to the loss
  log_gradient_step: 100 # how often to log gradient

  Components:
    GlobalEmbedding:
      learning_rate: *lr
      optimizer_group: "body" # learning rate based on the first component
      optimizer_type: "lion"
      warm_up: true
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0

    PET:
      learning_rate: *lr
      optimizer_group: "body"
      optimizer_type: "lion"
      warm_up: true
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0

    ObjectEncoder:
      learning_rate: *lr
      optimizer_group: "object_encoder"
      optimizer_type: "lion"
      warm_up: true
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0

    Classification:
      learning_rate: *lr
      optimizer_group: "classification"
      optimizer_type: "lion"
      warm_up: true
      loss_scale: 1.0 # loss coefficient
      include: true # whether to build in network
      include_cross_term: true # whether to calculate cross term loss
      loss_scale_cross_term: 1.0 # loss coefficient
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0

    Regression:
      learning_rate: *lr
      optimizer_group: "regression"
      optimizer_type: "lion"
      warm_up: true
      loss_scale: 1.0 # loss coefficient
      include: true # whether to build in network
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0

    Assignment:
      learning_rate: *lr
      optimizer_group: "assignment"
      optimizer_type: "lion"
      warm_up: true
      assignment_loss_scale: 1.0
      detection_loss_scale: 1.0
      focal_gamma: 1.0
      include: false # whether to build in network
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0

    Segmentation:
      learning_rate: *lr
      optimizer_group: "segmentation"
      optimizer_type: "lion"
      warm_up: true
      include: false # whether to build in network
      mask_loss_scale: 1.0 # loss coefficient
      dice_loss_scale: 1.0
      cls_loss_scale: 1.0
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      use_full_mask: true # whether to use full mask for training
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0

    GlobalGeneration:
      learning_rate: *lr
      optimizer_group: "generation"
      optimizer_type: "lion"
      warm_up: true
      include: false # whether to build in network
      loss_scale: 1.0 # loss coefficient
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      diffusion_steps: 20
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0

    ReconGeneration:
      learning_rate: *lr
      optimizer_group: "generation"
      optimizer_type: "lion"
      warm_up: true
      include: false # whether to build in network
      loss_scale: 1.0 # loss coefficient
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      diffusion_steps: 100
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0
      use_generation_result: false # whether to use the generation result as input for the next step

    TruthGeneration:
      learning_rate: *lr
      optimizer_group: "generation"
      optimizer_type: "lion"
      warm_up: true
      include: false # whether to build in network
      loss_scale: 1.0 # loss coefficient
      weight_decay: *wd
      decoupled_weight_decay: *decouple_wd
      diffusion_steps: 100
      freeze:
        type: none # ['partial', 'full', 'none', 'random']
        partial_freeze_components: [ ]
        random_freeze_fraction: 0.0

  ProgressiveTraining:
    stages:
      - name: "full_training"
        epoch_ratio: 1.0
        transition_ratio: 0.0
        loss_weights:
          generation-truth: [ 1.0, 1.0 ]
          generation-recon: [ 1.0, 1.0 ]
          classification: [ 1.0, 1.0 ]
          classification-noised: [ 1.0, 1.0 ]
          regression: [ 1.0, 1.0 ]
          assignment: [ 1.0, 1.0 ]
          segmentation: [ 1.0, 1.0 ]
        train_parameters:
          noise_prob: [ 1.0, 1.0 ]
          reco_attn_mask: [ 0.0, 0.0 ]
          ema_decay: [ 0.999, 0.999 ]

      - name: "test, make no sense"
        epoch_ratio: 0.2
        transition_ratio: 0.2  # ramp over 20% of this stage
        loss_weights:
          generation-truth: [ 1.0, 1.0 ]
          generation-recon: [ 1.0, 1.0 ]
          classification: [ 0.0, 0.0 ]
          classification-noised: [ 1.0, 1.0 ]
          regression: [ 0.0, 0.0 ]
          assignment: [ 0.0, 0.0 ]
          segmentation: [ 1.0, 1.0 ]
        train_parameters:
          noise_prob: [ 0.0, 0.0 ]
          reco_attn_mask: [ 0.0, 0.0 ]

  EarlyStopping:
    patience: 20
    min_delta: 0.0
    monitor: "val/loss"
    mode: "min"
    verbose: true

  FAMO:
    turn_on: false
    detailed_loss: false
    detailed_loss_list:
      - classification
      - classification-noised
      - generation-global
      - generation-event
      - generation-invisible
      - assignment
      - detection
      - regression
      - segmentation
    logits_bound: 1.0
    lr: 0.025

  EMA:
    enable: true               # Enable or disable EMA
    decay: 0.999               # EMA decay rate
    start_epoch: 0             # Start updating EMA after this epoch
    update_every_n_steps: 1    # Update EMA every N steps

    replace_model_after_load: false    # After loading checkpoint, copy EMA weights into model
    replace_model_at_end: false         # At training end, copy EMA weights into model (will not save EMA weights)
    # not implemented yet
    use_ema_during_training_eval: false # If you evaluate during training, use EMA

Dataset:
  dataset_limit: 1.0
  normalization_file: "/pscratch/sd/a/avencast/Event_Level_Analysis/Pretrain_Parquet/normalization_files/PreTrain_norm.pkl"
  balance_file: null
  val_split: [ 0.7, 1.0 ]

Metrics:
  # plotting bins: [bin_num, bin_min, bin_max]
  Generation-Binning:
    num_vectors: [ 21, -1, 20 ]

    "point cloud-log_energy": [ 50, -1, 10 ]
    "point cloud-log_pt": [ 50, -1, 10 ]
    "point cloud-eta": [ 50, -4, 4 ]
    "point cloud-phi": [ 50, -4, 4 ]
    "point cloud-btag": [ 2, 0, 1 ]
    "point cloud-isLepton": [ 2, 0, 1 ]
    "point cloud-charge": [ 3, -1, 1 ]

    "neutrino-log_mass": [ 10, -1, 1 ]
    "neutrino-log_pt": [ 50, 0, 7 ]
    "neutrino-eta": [ 50, -4, 4 ]
    "neutrino-phi": [ 50, -4, 4 ]
    "neutrino-btag": [ 2, 0, 1 ]
    "neutrino-isLepton": [ 2, 0, 1 ]
    "neutrino-charge": [ 3, -1, 1 ]