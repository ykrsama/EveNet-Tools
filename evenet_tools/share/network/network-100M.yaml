##############################################################################
# IMPORTANT NOTE:
# Please do not remove any existing fields in this file.
##############################################################################

Body:
  GlobalEmbedding:
    linear_block_type: GRU
    transformer_dim_scale: 2.0
    initial_embedding_dim: 16
    hidden_dim: 512
    normalization: LayerNorm
    linear_activation: GELU
    num_embedding_layers: 10
    dropout: 0.1
    skip_connection: true

  PET:
    num_feature_keep: 0
    feature_drop: 0.1
    hidden_dim: 512
    enable_local_embedding: true
    local_Krank: 3
    local_point_index:
      - 2
      - 3
    num_local_layer: 10
    num_layers: 20
    drop_probability: 0.1
    talking_head: false
    layer_scale: true
    layer_scale_init: 1.0e-05
    num_heads: 32
    mode: "all"
    dropout: 0.1

  ObjectEncoder:
    hidden_dim: 512
    position_embedding_dim: 32
    num_attention_heads: 16
    transformer_dim_scale: 2.0
    num_embedding_layers: 2
    num_encoder_layers: 2
    dropout: 0.1
    skip_connection: true
    encoder_skip_connection: true


Classification:
  num_classification_layers: 1
  hidden_dim: 512
  dropout: 0.1
  skip_connection: true
  num_attention_heads: 16


Regression:
  num_regression_layers: 1
  hidden_dim: 512
  dropout: 0.1
  skip_connection: true


Assignment:
  feature_drop: 0.1
  num_feature_keep: 0
  hidden_dim: 512
  split_symmetric_attention: true
  position_embedding_dim: 32
  num_attention_heads: 8
  transformer_dim_scale: 2.0
  num_jet_embedding_layers: 0
  num_jet_encoder_layers: 0
  num_detection_layers: 1
  dropout: 0.1
  combinatorial_scale: 0.0
  encode_event_token: false
  activation: GELU
  num_encoder_layers: 1 # At least 1 layer, otherwise detection will not work
  num_linear_layers: 1
  skip_connection: true
  encoder_skip_connection: true

Segmentation:
  projection_dim: 512
  num_heads: 8
  dropout: 0.1
  num_layers: 1
  num_queries: 4
  return_intermediate: true
  mask_mlp_layers: 1
  norm_before: false
  encode_event_token: false

GlobalGeneration:
  num_layers: 1
  num_resnet_layers: 1
  hidden_dim: 64
  resnet_dim: 64
  layer_scale_init: 1.0e-05
  feature_drop_for_stochastic_depth: 0.0
  activation: SILU
  dropout: 0.0

ReconGeneration:
  hidden_dim: 512
  num_layers: 1
  num_heads: 8
  dropout: 0.1
  layer_scale: true
  layer_scale_init: 1.0e-05
  drop_probability: 0.1
  feature_drop: 0.1
  extend_batch: true

TruthGeneration:
  hidden_dim: 512
  num_layers: 1
  num_heads: 8
  dropout: 0.1
  layer_scale: true
  layer_scale_init: 1.0e-05
  drop_probability: 0.1
  feature_drop: 0.1
  extend_batch: true
  neutrino_position_encode: true
  max_position_length: 36 # Must be larger than max object number
